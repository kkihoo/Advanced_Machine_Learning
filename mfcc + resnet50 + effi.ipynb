{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import os\n","import glob\n","import torch\n","import torchaudio\n","import torchaudio.transforms as T\n","import librosa\n","import IPython.display as ipd\n","import matplotlib.pyplot as plt\n","from torch.utils.data import DataLoader"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["train healthy   : 532 audios\n","train_pathology : 762 audios\n","test_healthy    : 100 audios\n","test_pathology  : 100 audios\n"]}],"source":["# 파일 개수 확인\n","train_healthy_paths = list(glob.glob('./SVD/train/healthy/*.wav'))\n","train_pathology_paths = list(glob.glob('./SVD/train/pathology/*.wav'))\n","test_healthy_paths = list(glob.glob('./SVD/test/healthy/*.wav'))\n","test_pathology_paths = list(glob.glob('./SVD/test/pathology/*.wav'))\n","print(f'train healthy   : {len(train_healthy_paths)} audios')\n","print(f'train_pathology : {len(train_pathology_paths)} audios')\n","print(f'test_healthy    : {len(test_healthy_paths)} audios')\n","print(f'test_pathology  : {len(test_pathology_paths)} audios')"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1713940029708,"user":{"displayName":"JH HONG","userId":"12244667994032889629"},"user_tz":-540},"id":"h-uEpQXO04co"},"outputs":[],"source":["# dataset 정의\n","def load_audios(paths):\n","\n","    paths = paths\n","    dataset = []\n","    for p in paths:\n","        name = os.path.basename(p)\n","        name = os.path.splitext(name)[0]\n","        waveform, sample_rate = torchaudio.load(p)\n","        dataset.append([waveform, sample_rate, name])\n","\n","    return dataset"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":23629,"status":"ok","timestamp":1713940055739,"user":{"displayName":"JH HONG","userId":"12244667994032889629"},"user_tz":-540},"id":"4QaYib0LA5rZ"},"outputs":[],"source":["# dataset 생성\n","h_train = load_audios(train_healthy_paths)\n","p_train = load_audios(train_pathology_paths)\n","h_test = load_audios(test_healthy_paths)\n","p_test = load_audios(test_pathology_paths)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1713940055740,"user":{"displayName":"JH HONG","userId":"12244667994032889629"},"user_tz":-540},"id":"m0B9pXWXBChr"},"outputs":[],"source":["# dataloader\n","loader_h_train = DataLoader(h_train, batch_size=1, shuffle=False)\n","loader_p_train = DataLoader(p_train, batch_size=1, shuffle=False)\n","loader_h_test = DataLoader(h_test, batch_size=1, shuffle=False)\n","loader_p_test = DataLoader(p_test, batch_size=1, shuffle=False)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# 데이터 증강 함수 정의\n","def augmentation(mfcc_tensor):\n","    # 주파수 마스킹\n","    freq_masking = T.FrequencyMasking(freq_mask_param=3)\n","    mfcc_tensor = freq_masking(mfcc_tensor)\n","    \n","    # 시간 마스킹\n","    time_masking = T.TimeMasking(time_mask_param=10)\n","    mfcc_tensor = time_masking(mfcc_tensor)\n","    \n","    return mfcc_tensor\n","# MFCC 이미지 생성 함수 정의\n","def create_mfcc_images(loader, label, t, augmentation_ratio=0):\n","    dir = f'./SVD/mfcc/{t}/{label}'\n","    os.makedirs(dir, exist_ok=True)\n","    \n","    augmented_count = int(len(loader) * augmentation_ratio)\n","    for i, data in enumerate(loader):\n","        waveform = data[0][0]\n","        sample_rate = data[1][0]\n","        name = data[2][0]\n","        \n","        # MFCC 변환\n","        mfcc_transform = T.MFCC(\n","            sample_rate=sample_rate,\n","            n_mfcc=13,\n","            melkwargs={\"n_fft\": 400, \"hop_length\": 160, \"n_mels\": 40}\n","        )\n","        mfcc = mfcc_transform(waveform)\n","        \n","        # 원본 이미지 저장\n","        plt.figure()\n","        librosa.display.specshow(mfcc[0].numpy(), x_axis='time')\n","        plt.colorbar()\n","        plt.title('MFCC')\n","        plt.xlabel('Time')\n","        plt.ylabel('MFCC Coefficients')\n","        plt.axis('off')\n","        plt.savefig(f'{dir}/{name}.png', bbox_inches='tight', pad_inches=0)\n","        plt.close()\n","        \n","        # 데이터 증강 및 증강 이미지 저장\n","        if i < augmented_count:\n","            augmented_mfcc = augmentation(mfcc)\n","            plt.figure()\n","            librosa.display.specshow(augmented_mfcc[0].numpy(), x_axis='time')\n","            plt.colorbar()\n","            plt.title('Augmented MFCC')\n","            plt.xlabel('Time')\n","            plt.ylabel('MFCC Coefficients')\n","            plt.axis('off')\n","            plt.savefig(f'{dir}/{name}_augmented.png', bbox_inches='tight', pad_inches=0)\n","            plt.close()\n"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":291375,"status":"ok","timestamp":1713940347106,"user":{"displayName":"JH HONG","userId":"12244667994032889629"},"user_tz":-540},"id":"70cGxHkYGTwi"},"outputs":[],"source":["# spectrogram image 생성 (5분 정도 소요)\n","create_mfcc_images(loader_h_train, 'healthy', 'train', augmentation_ratio=1)\n","create_mfcc_images(loader_p_train, 'pathology', 'train', augmentation_ratio=1)\n","create_mfcc_images(loader_h_test, 'healthy', 'test', augmentation_ratio=0)\n","create_mfcc_images(loader_p_test, 'pathology', 'test', augmentation_ratio=0)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1713940347107,"user":{"displayName":"JH HONG","userId":"12244667994032889629"},"user_tz":-540},"id":"c7z_Gwk0HBfs","outputId":"574ee91f-d5b0-47be-d642-562e4d6baa1d"},"outputs":[{"name":"stdout","output_type":"stream","text":["train healthy   : 1064 images\n","train_pathology : 1524 images\n","test_healthy    : 100 images\n","test_pathology  : 100 images\n"]}],"source":["# 파일 개수 확인\n","train_healthy_images = list(glob.glob('./SVD/mfcc/train/healthy/*.png'))\n","train_pathology_images = list(glob.glob('./SVD/mfcc/train/pathology/*.png'))\n","test_healthy_images = list(glob.glob('./SVD/mfcc/test/healthy/*.png'))\n","test_pathology_images = list(glob.glob('./SVD/mfcc/test/pathology/*.png'))\n","print(f'train healthy   : {len(train_healthy_images)} images')\n","print(f'train_pathology : {len(train_pathology_images)} images')\n","print(f'test_healthy    : {len(test_healthy_images)} images')\n","print(f'test_pathology  : {len(test_pathology_images)} images')"]},{"cell_type":"markdown","metadata":{"id":"RubAPLKTQSaY"},"source":["# classification"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":2820,"status":"ok","timestamp":1713940354476,"user":{"displayName":"JH HONG","userId":"12244667994032889629"},"user_tz":-540},"id":"BRjqtoJfSAB0"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/ki/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","import torchvision\n","from torchvision import transforms\n","from torchvision.datasets import ImageFolder\n","from torchvision.models import ResNet34_Weights\n","from sklearn.metrics import confusion_matrix\n","from torchaudio.transforms import MFCC\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":806,"status":"ok","timestamp":1713940356970,"user":{"displayName":"JH HONG","userId":"12244667994032889629"},"user_tz":-540},"id":"yCdp_2mJQa8M","outputId":"0c214449-1e5d-42cf-8b8a-f5e02909950f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset ImageFolder\n","    Number of datapoints: 2588\n","    Root location: ./SVD/mfcc/train\n","    StandardTransform\n","Transform: Compose(\n","               ToTensor()\n","               Resize(size=(256, 256), interpolation=bilinear, max_size=None, antialias=True)\n","           )\n","\n","class : index\n","{'healthy': 0, 'pathology': 1}\n","Dataset ImageFolder\n","    Number of datapoints: 200\n","    Root location: ./SVD/mfcc/test\n","    StandardTransform\n","Transform: Compose(\n","               ToTensor()\n","               Resize(size=(256, 256), interpolation=bilinear, max_size=None, antialias=True)\n","           )\n","\n","class : index\n","{'healthy': 0, 'pathology': 1}\n"]}],"source":["# trainset\n","train_path = './SVD/mfcc/train'\n","trainset = ImageFolder(root=train_path, transform=transforms.Compose([transforms.ToTensor(),\n","                                                                    transforms.Resize((256, 256))]))\n","print(trainset)\n","print(f'\\nclass : index\\n{trainset.class_to_idx}')\n","\n","# testset\n","test_path = './SVD/mfcc/test'\n","testset = ImageFolder(root=test_path, transform=transforms.Compose([transforms.ToTensor(),\n","                                                                    transforms.Resize((256, 256))]))\n","print(testset)\n","print(f'\\nclass : index\\n{testset.class_to_idx}')\n","\n","# dataloader\n","train_dataloader = DataLoader(trainset, batch_size=32, shuffle=True)\n","test_dataloader = DataLoader(testset, batch_size=16, shuffle=False)\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1713940363069,"user":{"displayName":"JH HONG","userId":"12244667994032889629"},"user_tz":-540},"id":"BZNJZxw3WJOL","outputId":"287aad94-0cda-4fda-c024-1e84cb73f3eb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using mps device\n"]}],"source":["# GPU\n","\n","device = torch.device(\"mps\")\n","print(f'Using {device} device')"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# 사전학습모델 불러오기\n","import numpy as np\n","from torchvision.models import resnet50, ResNet50_Weights, efficientnet_b0, EfficientNet_B0_Weights\n","\n","# ResNet50 모델\n","resnet50_model = resnet50(weights=ResNet50_Weights.DEFAULT)\n","resnet50_model.fc = nn.Sequential(\n","    nn.Dropout(p=0.5),\n","    nn.Linear(2048, 2)\n",")\n","# EfficientNet-B0 모델\n","efficientnet_b0_model = efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT)\n","efficientnet_b0_model.classifier = nn.Sequential(\n","    nn.Dropout(p=0.5),\n","    nn.Linear(1280, 2)\n",")\n","\n","# 모델, 손실함수, 옵티마이저\n","resnet50_model = resnet50_model.to(device)\n","efficientnet_b0_model = efficientnet_b0_model.to(device)\n","criterion = nn.CrossEntropyLoss()\n","resnet50_optimizer = optim.Adam(resnet50_model.parameters(), lr=0.0005)\n","efficientnet_b0_optimizer = optim.Adam(efficientnet_b0_model.parameters(), lr=0.0005)\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":275448,"status":"ok","timestamp":1713940654163,"user":{"displayName":"JH HONG","userId":"12244667994032889629"},"user_tz":-540},"id":"uG-YL3O3YehL","outputId":"03e17f19-2740-4497-909d-502f5e720071"},"outputs":[{"name":"stdout","output_type":"stream","text":["[epoch: 1 / batch:  10] loss: 0.6259, accuracy: 62.81%\n","[epoch: 1 / batch:  20] loss: 0.6094, accuracy: 65.00%\n","[epoch: 1 / batch:  30] loss: 0.4955, accuracy: 75.31%\n","[epoch: 1 / batch:  40] loss: 0.5073, accuracy: 74.69%\n","[epoch: 1 / batch:  50] loss: 0.4809, accuracy: 77.81%\n","[epoch: 1 / batch:  60] loss: 0.4675, accuracy: 78.12%\n","[epoch: 1 / batch:  70] loss: 0.4503, accuracy: 80.31%\n","[epoch: 1 / batch:  80] loss: 0.4231, accuracy: 78.75%\n","[epoch: 2 / batch:  10] loss: 0.4361, accuracy: 79.69%\n","[epoch: 2 / batch:  20] loss: 0.3762, accuracy: 81.88%\n","[epoch: 2 / batch:  30] loss: 0.4430, accuracy: 79.69%\n","[epoch: 2 / batch:  40] loss: 0.3863, accuracy: 83.75%\n","[epoch: 2 / batch:  50] loss: 0.2850, accuracy: 89.69%\n","[epoch: 2 / batch:  60] loss: 0.3121, accuracy: 86.56%\n","[epoch: 2 / batch:  70] loss: 0.3035, accuracy: 86.88%\n","[epoch: 2 / batch:  80] loss: 0.3940, accuracy: 82.81%\n","[epoch: 3 / batch:  10] loss: 0.3434, accuracy: 84.69%\n","[epoch: 3 / batch:  20] loss: 0.3258, accuracy: 85.94%\n","[epoch: 3 / batch:  30] loss: 0.3225, accuracy: 85.94%\n","[epoch: 3 / batch:  40] loss: 0.2678, accuracy: 89.69%\n","[epoch: 3 / batch:  50] loss: 0.2972, accuracy: 87.19%\n","[epoch: 3 / batch:  60] loss: 0.2473, accuracy: 90.00%\n","[epoch: 3 / batch:  70] loss: 0.2354, accuracy: 90.62%\n","[epoch: 3 / batch:  80] loss: 0.2269, accuracy: 90.00%\n","[epoch: 4 / batch:  10] loss: 0.1591, accuracy: 93.12%\n","[epoch: 4 / batch:  20] loss: 0.1633, accuracy: 93.44%\n","[epoch: 4 / batch:  30] loss: 0.2288, accuracy: 90.62%\n","[epoch: 4 / batch:  40] loss: 0.2076, accuracy: 92.19%\n","[epoch: 4 / batch:  50] loss: 0.1662, accuracy: 93.44%\n","[epoch: 4 / batch:  60] loss: 0.1829, accuracy: 91.25%\n","[epoch: 4 / batch:  70] loss: 0.1997, accuracy: 92.19%\n","[epoch: 4 / batch:  80] loss: 0.2011, accuracy: 90.62%\n","[epoch: 5 / batch:  10] loss: 0.1104, accuracy: 95.00%\n","[epoch: 5 / batch:  20] loss: 0.1008, accuracy: 96.88%\n","[epoch: 5 / batch:  30] loss: 0.1679, accuracy: 93.12%\n","[epoch: 5 / batch:  40] loss: 0.1189, accuracy: 95.94%\n","[epoch: 5 / batch:  50] loss: 0.1558, accuracy: 93.12%\n","[epoch: 5 / batch:  60] loss: 0.2528, accuracy: 90.00%\n","[epoch: 5 / batch:  70] loss: 0.1818, accuracy: 92.19%\n","[epoch: 5 / batch:  80] loss: 0.1414, accuracy: 94.69%\n","[epoch: 6 / batch:  10] loss: 0.1475, accuracy: 94.06%\n","[epoch: 6 / batch:  20] loss: 0.0778, accuracy: 96.25%\n","[epoch: 6 / batch:  30] loss: 0.0698, accuracy: 98.12%\n","[epoch: 6 / batch:  40] loss: 0.1054, accuracy: 96.25%\n","[epoch: 6 / batch:  50] loss: 0.0702, accuracy: 96.88%\n","[epoch: 6 / batch:  60] loss: 0.0990, accuracy: 95.94%\n","[epoch: 6 / batch:  70] loss: 0.0575, accuracy: 97.19%\n","[epoch: 6 / batch:  80] loss: 0.1102, accuracy: 95.94%\n","[epoch: 7 / batch:  10] loss: 0.0382, accuracy: 98.75%\n","[epoch: 7 / batch:  20] loss: 0.0704, accuracy: 98.44%\n","[epoch: 7 / batch:  30] loss: 0.0782, accuracy: 96.56%\n","[epoch: 7 / batch:  40] loss: 0.1091, accuracy: 95.31%\n","[epoch: 7 / batch:  50] loss: 0.1055, accuracy: 95.31%\n","[epoch: 7 / batch:  60] loss: 0.0726, accuracy: 96.88%\n","[epoch: 7 / batch:  70] loss: 0.0476, accuracy: 98.12%\n","[epoch: 7 / batch:  80] loss: 0.0993, accuracy: 96.25%\n","[epoch: 8 / batch:  10] loss: 0.0299, accuracy: 98.44%\n","[epoch: 8 / batch:  20] loss: 0.0539, accuracy: 98.12%\n","[epoch: 8 / batch:  30] loss: 0.0947, accuracy: 95.94%\n","[epoch: 8 / batch:  40] loss: 0.1065, accuracy: 95.94%\n","[epoch: 8 / batch:  50] loss: 0.0624, accuracy: 98.12%\n","[epoch: 8 / batch:  60] loss: 0.0820, accuracy: 97.50%\n","[epoch: 8 / batch:  70] loss: 0.0519, accuracy: 98.44%\n","[epoch: 8 / batch:  80] loss: 0.0790, accuracy: 97.50%\n","[epoch: 9 / batch:  10] loss: 0.0548, accuracy: 97.50%\n","[epoch: 9 / batch:  20] loss: 0.0321, accuracy: 99.06%\n","[epoch: 9 / batch:  30] loss: 0.0472, accuracy: 98.75%\n","[epoch: 9 / batch:  40] loss: 0.0341, accuracy: 99.06%\n","[epoch: 9 / batch:  50] loss: 0.0142, accuracy: 99.69%\n","[epoch: 9 / batch:  60] loss: 0.0209, accuracy: 99.38%\n","[epoch: 9 / batch:  70] loss: 0.0283, accuracy: 99.06%\n","[epoch: 9 / batch:  80] loss: 0.0268, accuracy: 98.75%\n","[epoch: 10 / batch:  10] loss: 0.0240, accuracy: 98.75%\n","[epoch: 10 / batch:  20] loss: 0.0231, accuracy: 99.38%\n","[epoch: 10 / batch:  30] loss: 0.0319, accuracy: 98.44%\n","[epoch: 10 / batch:  40] loss: 0.0422, accuracy: 98.44%\n","[epoch: 10 / batch:  50] loss: 0.0565, accuracy: 98.12%\n","[epoch: 10 / batch:  60] loss: 0.0380, accuracy: 98.12%\n","[epoch: 10 / batch:  70] loss: 0.1285, accuracy: 95.31%\n","[epoch: 10 / batch:  80] loss: 0.1067, accuracy: 96.88%\n","Finished Training\n","[epoch: 1 / batch:  10] loss: 0.0658, accuracy: 61.25%\n","[epoch: 1 / batch:  20] loss: 0.0588, accuracy: 69.06%\n","[epoch: 1 / batch:  30] loss: 0.0512, accuracy: 76.56%\n","[epoch: 1 / batch:  40] loss: 0.0500, accuracy: 74.38%\n","[epoch: 1 / batch:  50] loss: 0.0441, accuracy: 76.25%\n","[epoch: 1 / batch:  60] loss: 0.0429, accuracy: 80.94%\n","[epoch: 1 / batch:  70] loss: 0.0453, accuracy: 77.50%\n","[epoch: 1 / batch:  80] loss: 0.0456, accuracy: 79.38%\n","[epoch: 2 / batch:  10] loss: 0.0370, accuracy: 83.44%\n","[epoch: 2 / batch:  20] loss: 0.0290, accuracy: 88.75%\n","[epoch: 2 / batch:  30] loss: 0.0382, accuracy: 83.12%\n","[epoch: 2 / batch:  40] loss: 0.0427, accuracy: 80.31%\n","[epoch: 2 / batch:  50] loss: 0.0364, accuracy: 84.06%\n","[epoch: 2 / batch:  60] loss: 0.0327, accuracy: 87.50%\n","[epoch: 2 / batch:  70] loss: 0.0307, accuracy: 86.25%\n","[epoch: 2 / batch:  80] loss: 0.0306, accuracy: 85.31%\n","[epoch: 3 / batch:  10] loss: 0.0172, accuracy: 94.38%\n","[epoch: 3 / batch:  20] loss: 0.0206, accuracy: 91.25%\n","[epoch: 3 / batch:  30] loss: 0.0233, accuracy: 90.94%\n","[epoch: 3 / batch:  40] loss: 0.0272, accuracy: 88.75%\n","[epoch: 3 / batch:  50] loss: 0.0271, accuracy: 90.00%\n","[epoch: 3 / batch:  60] loss: 0.0227, accuracy: 91.25%\n","[epoch: 3 / batch:  70] loss: 0.0205, accuracy: 90.31%\n","[epoch: 3 / batch:  80] loss: 0.0202, accuracy: 92.19%\n","[epoch: 4 / batch:  10] loss: 0.0168, accuracy: 92.19%\n","[epoch: 4 / batch:  20] loss: 0.0213, accuracy: 89.69%\n","[epoch: 4 / batch:  30] loss: 0.0144, accuracy: 94.69%\n","[epoch: 4 / batch:  40] loss: 0.0160, accuracy: 93.44%\n","[epoch: 4 / batch:  50] loss: 0.0189, accuracy: 93.44%\n","[epoch: 4 / batch:  60] loss: 0.0108, accuracy: 95.31%\n","[epoch: 4 / batch:  70] loss: 0.0112, accuracy: 95.00%\n","[epoch: 4 / batch:  80] loss: 0.0138, accuracy: 95.00%\n","[epoch: 5 / batch:  10] loss: 0.0096, accuracy: 95.62%\n","[epoch: 5 / batch:  20] loss: 0.0097, accuracy: 94.69%\n","[epoch: 5 / batch:  30] loss: 0.0081, accuracy: 97.19%\n","[epoch: 5 / batch:  40] loss: 0.0085, accuracy: 97.19%\n","[epoch: 5 / batch:  50] loss: 0.0089, accuracy: 96.88%\n","[epoch: 5 / batch:  60] loss: 0.0129, accuracy: 94.69%\n","[epoch: 5 / batch:  70] loss: 0.0144, accuracy: 94.38%\n","[epoch: 5 / batch:  80] loss: 0.0143, accuracy: 94.38%\n","[epoch: 6 / batch:  10] loss: 0.0057, accuracy: 97.81%\n","[epoch: 6 / batch:  20] loss: 0.0086, accuracy: 97.50%\n","[epoch: 6 / batch:  30] loss: 0.0131, accuracy: 94.69%\n","[epoch: 6 / batch:  40] loss: 0.0069, accuracy: 97.50%\n","[epoch: 6 / batch:  50] loss: 0.0097, accuracy: 95.00%\n","[epoch: 6 / batch:  60] loss: 0.0099, accuracy: 97.19%\n","[epoch: 6 / batch:  70] loss: 0.0064, accuracy: 97.81%\n","[epoch: 6 / batch:  80] loss: 0.0057, accuracy: 98.44%\n","[epoch: 7 / batch:  10] loss: 0.0058, accuracy: 97.81%\n","[epoch: 7 / batch:  20] loss: 0.0073, accuracy: 97.50%\n","[epoch: 7 / batch:  30] loss: 0.0083, accuracy: 97.81%\n","[epoch: 7 / batch:  40] loss: 0.0081, accuracy: 98.12%\n","[epoch: 7 / batch:  50] loss: 0.0054, accuracy: 98.75%\n","[epoch: 7 / batch:  60] loss: 0.0071, accuracy: 97.19%\n","[epoch: 7 / batch:  70] loss: 0.0062, accuracy: 97.19%\n","[epoch: 7 / batch:  80] loss: 0.0052, accuracy: 97.81%\n","[epoch: 8 / batch:  10] loss: 0.0039, accuracy: 98.75%\n","[epoch: 8 / batch:  20] loss: 0.0048, accuracy: 99.06%\n","[epoch: 8 / batch:  30] loss: 0.0037, accuracy: 98.75%\n","[epoch: 8 / batch:  40] loss: 0.0021, accuracy: 98.75%\n","[epoch: 8 / batch:  50] loss: 0.0054, accuracy: 98.12%\n","[epoch: 8 / batch:  60] loss: 0.0029, accuracy: 98.75%\n","[epoch: 8 / batch:  70] loss: 0.0031, accuracy: 98.75%\n","[epoch: 8 / batch:  80] loss: 0.0028, accuracy: 99.06%\n","[epoch: 9 / batch:  10] loss: 0.0041, accuracy: 98.75%\n","[epoch: 9 / batch:  20] loss: 0.0052, accuracy: 97.50%\n","[epoch: 9 / batch:  30] loss: 0.0033, accuracy: 98.75%\n","[epoch: 9 / batch:  40] loss: 0.0076, accuracy: 99.06%\n","[epoch: 9 / batch:  50] loss: 0.0047, accuracy: 97.50%\n","[epoch: 9 / batch:  60] loss: 0.0077, accuracy: 96.56%\n","[epoch: 9 / batch:  70] loss: 0.0043, accuracy: 98.44%\n","[epoch: 9 / batch:  80] loss: 0.0074, accuracy: 96.88%\n","[epoch: 10 / batch:  10] loss: 0.0048, accuracy: 97.81%\n","[epoch: 10 / batch:  20] loss: 0.0037, accuracy: 98.75%\n","[epoch: 10 / batch:  30] loss: 0.0055, accuracy: 97.50%\n","[epoch: 10 / batch:  40] loss: 0.0030, accuracy: 98.44%\n","[epoch: 10 / batch:  50] loss: 0.0029, accuracy: 99.06%\n","[epoch: 10 / batch:  60] loss: 0.0025, accuracy: 99.38%\n","[epoch: 10 / batch:  70] loss: 0.0065, accuracy: 97.50%\n","[epoch: 10 / batch:  80] loss: 0.0049, accuracy: 98.12%\n","Finished Training\n"]}],"source":["# resnet training\n","for epoch in range(10):\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","    \n","    for i, data in enumerate(train_dataloader):\n","        images, labels = data[0].to(device), data[1].to(device)\n","        resnet50_optimizer.zero_grad()\n","        outputs = resnet50_model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        resnet50_optimizer.step()\n","        running_loss += loss.item()\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","        if (i + 1) % 10 == 0:\n","            accuracy = 100 * correct / total\n","            print(f'[epoch: {epoch+1} / batch: {i+1:3d}] loss: {running_loss/10:.4f}, accuracy: {accuracy:.2f}%')\n","            running_loss = 0.0\n","            correct = 0\n","            total = 0\n","\n","print('Finished Training')\n","\n","# efficient training \n","for epoch in range(10):\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    for i, data in enumerate(train_dataloader):\n","        images, labels = data[0].to(device), data[1].to(device)\n","        efficientnet_b0_optimizer.zero_grad()\n","        outputs = efficientnet_b0_model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        efficientnet_b0_optimizer.step()\n","        running_loss += loss.item()\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","        if (i + 1) % 10 == 0:\n","            accuracy = 100 * correct / total\n","            print(f'[epoch: {epoch+1} / batch: {i+1:3d}] loss: {running_loss/100:.4f}, accuracy: {accuracy:.2f}%')\n","            running_loss = 0.0\n","            correct = 0\n","            total = 0\n","\n","print('Finished Training')\n","\n","# ResNet50 모델 저장\n","resnet50_path = './SVD/resnet50_model.pth'\n","torch.save(resnet50_model.state_dict(), resnet50_path)\n","\n","# EfficientNet-B0 모델 저장\n","efficientnet_b0_path = './SVD/efficientnet_b0_model.pth'\n","torch.save(efficientnet_b0_model.state_dict(), efficientnet_b0_path)\n"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[83 17]\n"," [23 77]]\n","accuracy: 0.8000, recall: 0.7700, precision: 0.8191, f1: 0.7938\n"]}],"source":["\n","# 테스트 데이터에 대한 예측 결과 얻기\n","def confusion(resnet50_model, efficientnet_b0_model, loader, resnet50_weight, efficientnet_b0_weight):\n","    y_true = []\n","    ensemble_preds = []\n","    \n","    resnet50_model.eval()\n","    efficientnet_b0_model.eval()\n","    \n","    with torch.no_grad():\n","        for data in loader:\n","            images, labels = data[0].to(device), data[1].to(device)\n","            resnet50_outputs = resnet50_model(images)\n","            efficientnet_b0_outputs = efficientnet_b0_model(images)\n","            \n","            # Softmax 함수 적용하여 정규화\n","            resnet50_probabilities = nn.functional.softmax(resnet50_outputs, dim=1)\n","            efficientnet_b0_probabilities = nn.functional.softmax(efficientnet_b0_outputs, dim=1)\n","            \n","            ensemble_probabilities = (resnet50_weight * resnet50_probabilities) + (efficientnet_b0_weight * efficientnet_b0_probabilities)\n","            preds = torch.argmax(ensemble_probabilities, dim=1)\n","            \n","            y_true.extend(labels.tolist())\n","            ensemble_preds.extend(preds.tolist())\n","    \n","    cm = confusion_matrix(y_true, ensemble_preds)\n","    return cm\n","\n","# 가중치 탐색 함수\n","def weight_search(resnet50_model, efficientnet_b0_model, loader):\n","    best_accuracy = 0\n","    best_weights = None\n","    best_metrics = None\n","\n","    # 가중치 범위 설정 (0.3 ~ 0.7)\n","    weight_range = np.arange(0.3, 0.71, 0.05)\n","    for resnet50_weight in weight_range:\n","        efficientnet_b0_weight = 1 - resnet50_weight\n","        # 주어진 가중치로 평가\n","        cm = confusion(resnet50_model, efficientnet_b0_model, loader, resnet50_weight, efficientnet_b0_weight)\n","        accuracy, recall, precision, f1 = metrics(cm)\n","        # 최고 accuracy 갱신\n","        if accuracy > best_accuracy:\n","            best_accuracy = accuracy\n","            best_weights = (resnet50_weight, efficientnet_b0_weight)\n","            best_metrics = (accuracy, recall, precision, f1)\n","\n","    return best_weights, best_metrics\n","\n","# metrics 계산 함수\n","def metrics(cm):\n","    tn, fp, fn, tp = cm.ravel()\n","    accuracy = (tp + tn) / (tp + tn + fp + fn)\n","    recall = (tp) / (tp + fn)\n","    precision = (tp) / (tp + fp)\n","    f1 = (2 * recall * precision) / (recall + precision)\n","\n","    return accuracy, recall, precision, f1\n","\n","# 최적 가중치 탐색\n","best_weights, best_metrics = weight_search(resnet50_model, efficientnet_b0_model, test_dataloader)\n","best_resnet50_weight, best_efficientnet_b0_weight = best_weights\n","\n","# 최적 가중치로 confusion matrix 계산\n","cm = confusion(resnet50_model, efficientnet_b0_model, test_dataloader, best_resnet50_weight, best_efficientnet_b0_weight)\n","\n","# 최적 가중치와 confusion matrix 출력\n","print(f\"Best Weights: ResNet50 = {best_resnet50_weight:.2f}, EfficientNet-B0 = {best_efficientnet_b0_weight:.2f}\")\n","print(cm)\n","\n","# 최적 가중치로 계산된 metrics 출력\n","accuracy, recall, precision, f1 = best_metrics\n","print(f'accuracy: {accuracy:.4f}, recall: {recall:.4f}, precision: {precision:.4f}, f1: {f1:.4f}')"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}
